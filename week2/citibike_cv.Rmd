---
title: "Week 2, HW4: Cross Validation and Model Selection"
author: "Amanda Rodriguez"
date: "June 21, 2018"
output: html_document
---

Using the bikeshare data, select which order of polynomial 1:10 best uses temperature to predict total bike ridership.
```{r}
#	Create a data frame with one row for each day, the number of trips taken on that day, and the minimum temperature on that day.
library(tidyverse)
load("~/coursework/week1/trips.RData")
trips_per_day <- trips %>% group_by(ymd) %>% summarise(num_trips = n()) 
trips_by_weather <- left_join(trips_per_day, weather) %>% filter(date > 0)

# Split the data into a randomly selected training and test set, as in the above exercise, with 80% of the data for training the model and 20% for testing.
set.seed(101) 

samp <- sample.int(n = nrow(trips_by_weather), size = floor(.8*nrow(trips_by_weather)), replace = F)
train <- trips_by_weather[samp, ]
test  <- trips_by_weather[-samp, ]

# Fit a model using lm to predict the number of trips as a (linear) function of the minimum temperature, and evaluate the fit on the training and testing data sets. Do this first visually by plotting the predicted and actual values as a function of the minimum temperature. Then do this with R^2 and RMSE on both the training and test sets. You'll want to use the predict and cor functions for this.
lm_train <- train %>%  
  lm(formula = num_trips ~ tmin)

test$y_hat <- predict(lm_train, newdata = test)
test$r_squared <- cor(test$num_trips,test$y_hat)^2

train$y_hat <- predict(lm_train, newdata = train)
train$r_squared <-cor(train$num_trips,train$y_hat)^2

library(modelr)
rmse(lm_train, test)
rmse(lm_train, train)

test %>% ggplot(aes(x = tmin, y = num_trips)) + geom_line(aes(y = y_hat)) + geom_point()

```

```{r}
# Now automate this, extending the model to higher-order polynomials with a for loop over the degree k. For each value of k, fit a model to the training data and save the R^2 on the training data to one vector and test vector to another. Then plot the training and test R^2 as a function of k. What value of k has the best performance?

RMSEs <- c()
r_squared_poly <- c()
for (i in 1:5) { 
reg_poly <- train %>% lm(formula = num_trips ~ poly(tmin, i))
  
  #predict y
test$reg_poly_hat <- predict(reg_poly, newdata = test)
RMSE <- rmse(reg_poly, test)
r_squared <- cor(test$num_trips,test$reg_poly_hat)^2
test$RMSEs_poly[i] <- RMSE
test$r_squared_poly[i] <- r_squared

train$reg_poly_hat <- predict(reg_poly, newdata = train)
RMSE <- rmse(reg_poly, train)
r_squared <- cor(train$num_trips,train$reg_poly_hat)^2
train$RMSEs_poly[i] <- RMSE
train$r_squared_poly[i] <- r_squared
} 

df1 <- data.frame(train$r_squared_poly, test$r_squared_poly) %>% head(5)
df2 <- df1 %>% mutate(poly_degree = seq.int(nrow(df1)))
df2 %>% ggplot(aes(x = poly_degree)) + geom_line(aes(y = train.r_squared_poly)) + geom_line(aes(y = test.r_squared_poly), color = "red")
```

```{r}

# Finally, fit one model for the value of k with the best performance in 5), and plot the actual and predicted values for this model.
test %>% ggplot(aes(x = tmin, y = num_trips)) + geom_line(aes(y = reg_poly_hat)) + geom_point()

```