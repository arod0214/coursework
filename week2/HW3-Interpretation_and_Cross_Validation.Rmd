---
title: "Week 2, HW3: Interpretation and Cross Validation"
author: "Amanda Rodriguez"
date: "June 20, 2018"
output: html_document
---

## Let's return to the orange juice dataset and investigate how store demographics are related to demand.
```{r}
# Take the "fully interacted" model from HW2 (logmove ~ log(price)*brand*feat) and add in the store demographics as linear features (e.g. + demo1 + demo2+.). 
library(tidyverse)
oj <- read_csv("oj.csv")
model <- oj %>% lm(formula = logmove ~ log(price)*brand*feat + AGE60 + EDUC + ETHNIC + INCOME + HHLARGE + WORKWOM + HVAL150 + SSTRDIST + SSTRVOL + CPDIST5 + CPWVOL5)

#	What demographics are significantly (t>2) related to demand? 
# All demographics.

#	How much did the adjusted R-squared improve with the addition of these variables?
oj %>% lm(formula = logmove ~ log(price)*brand*feat) %>% summary() # R-squared = 0.5352
# R-squared with demographics = 0.5848

```

## Let's focus on two variables HVAL150 ("percent of HHs with homes >$150K") and one of your choosing. 
```{r}
#	What are the means and percentiles of each of these variables? HINT: summary(oj$HVAL150)
summary(oj$HVAL150)
summary(oj$ETHNIC)

#	Using your coefficient estimates from the regression in 1b:
#	If we move from the median value of HVAL150 to the 75th percentile (3rd quartile), how much does log(quantity) change each week on average?
#HINT: using coef(reg_output)["var_name"] exports the coefficient on "var_name" from the regression model "reg_output".  
#Similarly, summary(df$var_name) will output a bunch of summary statistics for the variable var_name in data frame df.  Using summary(df$var_name)["3rd Qu."] will take the level of the 3rd quantile from the summary of var_name.  
#Because we estimate things in logs you'll want to take the exponent of everything.
log((summary(oj$HVAL150)["3rd Qu."] - summary(oj$HVAL150)["Median"]) * coef(model)["HVAL150"])

#	If we move from the median value of ETHNIC to the 75th percentile (3rd quartile), how much does log(quantity) change each week on average?
log((summary(oj$ETHNIC)["3rd Qu."] - summary(oj$ETHNIC)["Median"]) * coef(model)["ETHNIC"]) 

#	Base on this analysis, which is the more important predictor of demand?
#HVAL150 

# Now let's see if these variables impact price sensitivity. Add two interaction terms (with logprice) to the model to test this. 
#	What are the coefficients on the interaction terms? 
model_price_interaction <- oj %>% lm(formula = logmove ~ log(price)*brand*feat + ETHNIC * log(price) + HVAL150 * log(price)) %>% summary()
coef(model_price_interaction)["log(price):ETHNIC"]
coef(model_price_interaction)["log(price):HVAL150"]

#	Recall, positive values indicate lower price sensitivity and negative values indicate greater price sensitivity. Do your estimates make sense based on your intuition?
# Ethnic is more sensitive to price 

#	What are the coefficient estimates on the constants HVAL150 and your variable of choice? How do they compare to your regression from 1b?
coef(model_price_interaction)["ETHNIC"]
coef(model_price_interaction)["HVAL150"]
coef(model)["ETHNIC"]
coef(model)["HVAL150"]

# In the previous model, HVAL150 had a positive coefficient


#	Similar to 2b, if we move from the median value of each variable to the 3rd quartile, how much does elasticity change? Based on this, which is more important to price sensitivity?
log((summary(oj$ETHNIC)["3rd Qu."] - summary(oj$ETHNIC)["Median"]) * coef(model_price_interaction)["log(price):ETHNIC"])
log((summary(oj$HVAL150)["3rd Qu."] - summary(oj$HVAL150)["Median"]) * coef(model_price_interaction)["log(price):HVAL150"])
```

## Tuna fish question! Create make a new dataframe which takes the previous week's prices as a variable on the same line as the current week.  This would enable you to see if there is intertemporal substitution.
```{r}
#	There are going to be a couple of steps.  First is creating a new dataframe which is like the old one except that the week variable will change by a single week
df1 <-oj
df1$week<-df1$week+1  
# df1 now has NEXT week and not the current one.  If we merge this by #weeks now, this is last week's price (e.g., "lagged price").
myvars <- c("price", "week", "brand","store")
df1 <- df1[myvars]
lagged <- merge(oj, df1, by=c("brand","store","week")) 

# Investigate lagged and rename the lagged store values needed for a lagged price within the same store
colnames(lagged)[18] <- "lagged_price"
colnames(lagged)[6] <- "price"

#	Now run a regression with this week's log(quantity) on current and last week's price.
reg_lagged <- lagged %>% lm(formula = logmove ~ log(price)*brand + feat + log(lagged_price))

#	What do you notice about the previous week's elasticity?  Does this make sales more or less attractive from a profit maximization perspective?  Why?
```
  
## In the last assignment you calculated the MSE on a test set.  Let's expand that code to include 5-fold cross validation.   
```{r}
#	Create 5 partitions of the data of equal size.
set.seed(13)
folds <- 5
random_oj <- lagged[sample(nrow(lagged)),]
random_oj$rand_obs <- seq(1,nrow(random_oj))
random_oj$partition <- random_oj$rand_obs %% folds + 1
MSEs <- c(1:folds)

for (i in 1:folds) {
  oj_test1 <- random_oj[which(random_oj$partition == i),]
  oj_train1 <- anti_join(random_oj,oj_test1)
  reg1 <- oj_train1 %>% lm(formula = logmove ~ log(price) + brand + feat + AGE60 + EDUC + ETHNIC + INCOME + HHLARGE + WORKWOM + HVAL150 + SSTRDIST + SSTRVOL + CPDIST5 + CPWVOL5 + log(price)*ETHNIC + log(price)*HVAL150 + log(lagged_price))
  
  #predict y
  oj_test1$logmove_hat <- predict(reg1, newdata = oj_test1)
  MSE <- mean((oj_test1$logmove_hat - oj_test1$logmove)^2)
  MSEs[i] <- MSE
}
mean(MSEs)

#	Create 5 training datasets using 80% of the data for each one.  This can be done multiple ways (e.g., "appending" the data together using rbind, randomly creating partitions and sub-setting data according to them, etc.)

#	Estimate a complex model using OLS which includes price, featured, brand, brand*price and lagged price, all the sociodemographic variables and interactions of EDUC and HHSIZE with price on each of the training sets then the MSE on the test sets using the predict command.

#	Calculate the MSE for each run of the model by averaging across all the MSEs.
```

```{r}
#	Now take that same model from (4) and estimate it with LASSO.  Here is some relevant code:
library(glmnet)
lagged$log.price <- log(lagged$price)
lagged$log.lagged_price <- log(lagged$lagged_price)

x <- model.matrix(~ log(price) + feat + brand + brand*log(price) + . +log(lagged_price) , data= lagged) 
y <- as.numeric(as.matrix(lagged$logmove)) 
set.seed(720) 
lasso_v1 <- glmnet(x, y, alpha=1)
plot(lasso_v1)
coef(lasso_v1, s=lasso_v1$lambda.min)

#The cross validated version of the model (with some different objects) is this one: 
#lasso_v1 <- cv.glmnet(x, y, alpha=1)
#cvfit$lambda.min
#coef(cvfit, s = "lambda.min")

library(tidyverse)
oj_spread <- oj %>% select(price, week, brand,store)
oj_spread <- oj_spread %>% spread(brand, price)
oj_merge <- merge(oj, oj_spread)
colnames(oj_merge)[20] <- "trop_price"
colnames(oj_merge)[19] <- "minmaid_price"
colnames(oj_merge)[18] <- "dom_price"
```